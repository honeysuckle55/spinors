{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hadamard Time\n\nOn a given system and hardware configuration, times Hadamard product for\nincreasing mesh grid sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport sys\nimport timeit\nimport math\nsys.path.insert(0, os.path.abspath('../..'))  # Adds project root to the PATH\n\nimport numpy as np\nimport torch\nfrom scipy.stats import median_abs_deviation as mad\nfrom tqdm import tqdm\n\nfrom spinor_gpe.pspinor import pspinor as spin\nfrom spinor_gpe.pspinor import tensor_tools as ttools\n\ntorch.cuda.empty_cache()\n\n\ndef closestDivisors(n):\n    \"\"\"Return the two largest divisors that are closest together.\"\"\"\n    b = round(math.sqrt(n))\n    while n % b > 0:\n        b -= 1\n    return b, n // b\n\n\nclosest = np.vectorize(closestDivisors)\n\na = np.logspace(12, 24, 25, base=2.0, dtype=int)\nprint(a)\nprint(closest(a))\nclose = closest(a)\ngrids = [(a, b) for a, b in zip(close[0], close[1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_grids = len(grids)\nmeas_times = [[0] for i in range(n_grids)]\nrepeats = np.zeros(n_grids)\nsize = np.zeros(n_grids)\n\n\nDATA_PATH = 'benchmarks/Bench_001'  # Default data path is in the /data/ folder\n\nW = 2 * np.pi * 50\nATOM_NUM = 1e2\nOMEG = {'x': W, 'y': W, 'z': 40 * W}\nG_SC = {'uu': 1, 'dd': 1, 'ud': 1.04}\n\nDEVICE = 'cuda'\nCOMPUTER = 'Acer Aspire'\n\nps = spin.PSpinor(DATA_PATH, overwrite=True,\n                  atom_num=ATOM_NUM, omeg=OMEG, g_sc=G_SC,\n                  pop_frac=(0.5, 0.5), r_sizes=(8, 8),\n                  mesh_points=grids[0])\n\nfor i, grid in tqdm(enumerate(grids)):\n    print(f'{i}/{len(grids)}')\n    try:\n        psi = [torch.rand(grid, device=DEVICE, dtype=torch.complex128),\n               torch.rand(grid, device=DEVICE, dtype=torch.complex128)]\n        op = [torch.rand(grid, device=DEVICE, dtype=torch.float64),\n              torch.rand(grid, device=DEVICE, dtype=torch.float64)]\n\n        stmt = \"\"\"x = [o * p for o, p in zip(op, psi)]\"\"\"\n\n        timer = timeit.Timer(stmt=stmt, globals=globals())\n\n        N = timer.autorange()[0]\n        if N < 10:\n            N *= 10\n        vals = timer.repeat(N, 1)\n        meas_times[i] = vals\n        repeats[i] = N\n        size[i] = np.log2(np.prod(grid))\n\n        torch.cuda.empty_cache()\n    except RuntimeError as ex:\n        print(ex)\n        break\n\nmedian = np.array([np.median(times) for times in meas_times])\nmed_ab_dev = np.array([mad(times, scale='normal') for times in meas_times])\n\ntag = COMPUTER + '_' + DEVICE + '_had'\nnp.savez('data\\\\' + tag, computer=COMPUTER, device=DEVICE,\n         size=size, n_repeats=repeats, med=median, mad=med_ab_dev)\n\nnp.save(ps.paths['data'] + '..\\\\' + tag, np.array(meas_times, dtype='object'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}